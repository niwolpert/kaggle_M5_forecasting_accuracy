{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":7,"outputs":[{"output_type":"stream","text":"/kaggle/input/m5-preloaded-data/sales_train_val.csv\n/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\n/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\n/kaggle/input/m5-forecasting-accuracy/calendar.csv\n/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nimport time\nimport math\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\n\nfrom sklearn import preprocessing, metrics\n\nimport subprocess\nimport sys\n# for uninstalled packages, use:\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Memory optimization\n\n# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n# Modified by @Vopani\n\n# to support timestamp type, categorical type and to add option to use float16\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify path to raw data\npath_data = '/kaggle/input/m5-forecasting-accuracy/'","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for reading and transforming data\n\ndef read_and_transform(start_day):\n    \n    dtypes_calendar={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n             \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n            \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n    dtypes_sell_prices = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n    \n    # transform categorical variables into int16 format since it saves memory\n\n    sell_prices = pd.read_csv(path_data + 'sell_prices.csv', dtype = dtypes_sell_prices)\n    \n    for col, col_dtype in dtypes_sell_prices.items():\n        if col_dtype == \"category\":\n            sell_prices[col] = sell_prices[col].cat.codes.astype(\"int16\")\n            sell_prices[col] -= sell_prices[col].min()\n\n    calendar = pd.read_csv(path_data + 'calendar.csv', dtype = dtypes_calendar)\n\n    calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n    for col, col_dtype in dtypes_calendar.items():\n        if col_dtype == \"category\":\n            calendar[col] = calendar[col].cat.codes.astype(\"int16\")\n            calendar[col] -= calendar[col].min()\n\n    # start from given day\n\n    numcols = [f\"d_{day}\" for day in range(start_day,1914)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    sales_train_val = pd.read_csv(path_data + 'sales_train_validation.csv', usecols = catcols + numcols, dtype = dtype)\n    \n    # transform categorical columsn to integer to save memory\n    for col in catcols:\n        if col != \"id\":\n            sales_train_val[col] = sales_train_val[col].cat.codes.astype(\"int16\")\n            sales_train_val[col] -= sales_train_val[col].min()\n\n    print('###### Transforming into melted and merged data format...')\n\n    ### melt sales dataframe\n    \n    id_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    sales_melt = sales_train_val.melt(\n        id_vars=id_columns, value_vars = sales_train_val.drop(id_columns, axis=1).columns, \n        var_name='d', \n        value_name='sales'\n    )\n    sales_melt = reduce_mem_usage(sales_melt)\n\n    # get product table\n    product_infos = sales_train_val[id_columns].drop_duplicates()\n\n    del sales_train_val\n\n    ### melt submission dataframe\n\n    submission = pd.read_csv(path_data + 'sample_submission.csv')\n\n    sub_cols = submission.drop(['id'], axis=1).columns\n\n    submission_melt = submission.melt(\n        id_vars = ['id'],\n        value_vars = sub_cols, \n        var_name = 'd',\n        value_name = 'sales')\n\n    del submission\n\n    ### convert submission df to appropiate day format\n    submission_melt['d'] = submission_melt['d'].str.replace('F','')\n    submission_melt['d'] = pd.to_numeric(submission_melt['d'], errors='coerce')\n\n    submission_melt.loc[submission_melt[\"id\"].str.contains(\"validation\"), 'd'] += 1913\n    submission_melt.loc[submission_melt[\"id\"].str.contains(\"evaluation\"), 'd'] += 1941\n\n    submission_melt = submission_melt.applymap(str)\n    submission_melt['d'] = 'd_'+ submission_melt['d'].astype(str)\n\n    submission_melt.sales = submission_melt.sales.astype('float32')\n\n    submission_melt=reduce_mem_usage(submission_melt)\n\n    ### split up into training, validation and test data set\n    # - submission consisting of:\n    #   *sales from day 1914-1941 (used for the leaderbord)\n    #   *sales fro day 1942-1970 (used for final score)\n\n    # merge product infos on submission file\n\n    # temporarily separate test dataframes\n    df_submission1 = submission_melt[submission_melt[\"id\"].str.contains(\"validation\")]\n    df_submission2 = submission_melt[submission_melt[\"id\"].str.contains(\"evaluation\")]\n\n    del submission_melt\n\n    # merge with product table\n    # to do that we have to temporarily rename values in the id column\n    df_submission2[\"id\"] = df_submission2['id'].str.replace(\"_evaluation\", \"_validation\")\n    df_submission1 = df_submission1.merge(product_infos, how=\"left\", on=\"id\")\n    df_submission2 = df_submission2.merge(product_infos, how=\"left\", on=\"id\")\n    df_submission2[\"id\"] = df_submission2[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n    df_submission1['part'] = 'public_leaderboard'\n    df_submission2['part'] = 'private_leaderboard'\n\n    # for the moment only use public leaderboard data\n    #df_submission = pd.concat([df_submission1, df_submission2], axis=0)\n    df_submission = df_submission1.copy()\n\n    df_submission=reduce_mem_usage(df_submission)\n\n    del product_infos, df_submission1, df_submission2\n    gc.collect()\n\n    ### Merge calendar data\n    # drop time features (own ones will be added)\n    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n    df_train_val = sales_melt.merge(calendar, how=\"left\", on=\"d\")\n    del sales_melt\n    df_submission = df_submission.merge(calendar, how=\"left\", on=\"d\")\n\n    del calendar\n\n    # Merge sell price data\n    #df_train_val = df_train_val.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'], how=\"left\")\n    #df_submission = df_submission.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'], how=\"left\")\n    df_train_val = df_train_val.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'])\n    df_submission = df_submission.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'])\n\n    del sell_prices\n\n    df_train_val['part'] = 'train_val'\n    df_submission = df_submission[df_train_val.columns]\n\n    ### Merge trains and submission dfs\n    df_train_val_test = pd.concat([df_train_val, df_submission], axis=0)\n\n    del df_train_val, df_submission\n\n    ### add time-features\n    df_train_val_test['date'] = pd.to_datetime(df_train_val_test.date, format=\"%Y-%m-%d %H:%M:%S\")\n    df_train_val_test['year'] = df_train_val_test.date.dt.year\n    df_train_val_test['month'] = df_train_val_test.date.dt.month.astype('int8')\n    df_train_val_test['day'] = df_train_val_test.date.dt.day.astype('int8')\n    df_train_val_test['day_of_month'] = df_train_val_test.date.dt.day.astype('int8')\n    df_train_val_test['weekday'] = df_train_val_test.date.dt.weekday.astype('int8')\n    df_train_val_test['hour'] = df_train_val_test.date.dt.hour.astype('int8')\n    df_train_val_test['weekend'] = np.where((df_train_val_test.weekday) > 4,1,0).astype('int8')\n    df_train_val_test['quarter'] = df_train_val_test.date.dt.quarter.astype('int8')\n    df_train_val_test['weekofyear'] = df_train_val_test.date.dt.weekofyear.astype('int8')\n    df_train_val_test['week'] = df_train_val_test.date.dt.week.astype('int8')\n    df_train_val_test['dayofweek'] = df_train_val_test.date.dt.dayofweek.astype('int8')\n\n    df_train_val_test = reduce_mem_usage(df_train_val_test)\n\n    gc.collect()\n    \n    df_train_val_test.sales = df_train_val_test.sales.astype('int16')\n    df_train_val_test.sell_price = df_train_val_test.sell_price.astype('float16')\n    df_train_val_test['snap_CA'] = df_train_val_test.snap_CA.astype('int8')\n    df_train_val_test['snap_TX'] = df_train_val_test.snap_TX.astype('int8')\n    df_train_val_test['snap_WI'] = df_train_val_test.snap_WI.astype('int8')\n    \n    return df_train_val_test","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First data inspection"},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices = pd.read_csv(path_data + 'sell_prices.csv')\ncalendar = pd.read_csv(path_data + 'calendar.csv')\nsales_train_val = pd.read_csv(path_data + 'sales_train_validation.csv')\nsubmission = pd.read_csv(path_data + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar[['event_name_1','event_type_1','event_name_2','event_type_2']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val.cat_id.unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count number of zero and nonzero elements for each feature\nnonzero_total = sales_train_val.drop(columns=['id','item_id','dept_id','cat_id','store_id','state_id'], axis=1).astype(bool).sum(axis=0).sort_values(ascending = False)\nnonzero_perc = nonzero_total/sales_train_val.shape[0]\nnonzero = pd.concat([nonzero_total, nonzero_perc], axis=1, keys=['Total', 'Percent'])\n\n# display featues with most and fewest nonzero elements\nprint('Nonzero elements by feature: ')\nnonzero.head(10).append(nonzero.tail(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sell_prices, calendar, sales_train_val, submission, nonzero\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = pd.read_csv(path_data + 'sales_train_validation.csv')\nmean_per_cat = sales_train_val.groupby('cat_id').agg('mean').T.reset_index().drop('index', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_per_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_sales_sc = go.Scatter(x=daily_sales['date'], y=daily_sales['sales'])\nlayout = go.Layout(title='Daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=[daily_sales_sc], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_daily_sales_sc = []\nfor store in store_daily_sales['store'].unique():\n    current_store_daily_sales = store_daily_sales[(store_daily_sales['store'] == store)]\n    store_daily_sales_sc.append(go.Scatter(x=current_store_daily_sales['date'], y=current_store_daily_sales['sales'], name=('Store %s' % store)))\n\nlayout = go.Layout(title='Store daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=store_daily_sales_sc, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read some data temporarily for EDA\nsales_train_val = pd.read_csv(path_data + 'sales_train_validation.csv')\nmean_per_cat = sales_train_val.groupby('cat_id').agg('mean').T.reset_index().drop('index', axis=1)\nax = mean_per_cat.plot(figsize=(13, 6))\nax.set_xlabel(\"day\", fontsize=15)\nax.set_ylabel(\"avg sells\", fontsize=15)\nax.tick_params(labelsize=15)\ndel sales_train_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> cyclic variations\n\n-> overall trend for increase\n\n-> days with sharp drop in sells"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge some data temporarily for EDA\nstart_day = 350 #902  # 350\ndf_train_val_test = read_and_transform(start_day)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total number of sells over the whole time span\nsales_by_date = df_train_val_test[df_train_val_test.part=='train_val'][['date','sales']].groupby('date').sum().reset_index()\nplt.figure(figsize=(10,5))\nplt.plot(sales_by_date.date, sales_by_date.sales)\n_ =plt.xlabel('date', fontsize=15)\n_ =plt.ylabel('sales in total', fontsize=15)\n_ =plt.tick_params(labelsize=12)\n_ =plt.title('Total number of sales over train-val set', fontsize=16)\nfor year in df_train_val_test.year.unique().tolist():\n    day_christmas = pd.to_datetime(str(year) + '-12-25')\n    day_ny = pd.to_datetime(str(year) + '-01-01')\n    day_id = pd.to_datetime(str(year) + '-07-04')\n    if day_christmas<=sales_by_date.date.max():\n        c = plt.scatter(pd.to_datetime(day_christmas), sales_by_date.loc[sales_by_date.date==day_christmas, 'sales'].iloc[0], s=70, linewidths= 2, edgecolor=\"red\", facecolor='none', zorder=10)\n    if (day_ny>=sales_by_date.date.min()) & (day_ny<=sales_by_date.date.max()):\n        ny = plt.scatter(pd.to_datetime(day_ny), sales_by_date.loc[sales_by_date.date==day_ny, 'sales'].iloc[0], s=70, linewidths= 2, color=\"green\", facecolor='none', zorder=10)\n    if day_christmas<=sales_by_date.date.max():\n        inday = plt.scatter(pd.to_datetime(day_id), sales_by_date.loc[sales_by_date.date==day_id, 'sales'].iloc[0], s=70, linewidths= 2, edgecolor=\"orange\", facecolor='none', zorder=10)\nplt.legend([c, ny, inday], ['Christmas', 'New Year', 'July 4'], fontsize=12)\n#del sales_by_date","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Christmas and New Year are indeed minima in number of sales, plus probably Thanksgiving. Let's check..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# show dates with fewest sells\ndf_grouped = df_train_val_test.pivot_table(index=('month','day'),values='sales',aggfunc='sum').reset_index()\ndf_grouped.sort_values(by=['sales'], ascending=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, besides Christmas and New Year's (and 29th February, which is a leap day), Thanksgiving has fewest sells (we'll have to specify the date per year since the precise date varies)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%whos DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_grouped, df_train_val_test, mean_per_cat, sales_by_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = pd.read_csv(path_data + 'sales_train_validation.csv')\nd_cols = [col for col in sales_train_val.columns if 'd_' in col]\nid_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\ncols = ['id','item_id','dept_id','cat_id','store_id','state_id']\ncols.extend(f\"d_{day}\" for day in range(800,1914))\nsales_melt = sales_train_val[cols].melt(\n    id_vars=id_columns, value_vars = sales_train_val[cols].drop(id_columns, axis=1).columns, \n    var_name='d', \n    value_name='sales'\n)\nsell_prices = pd.read_csv(path_data + 'sell_prices.csv')\ncalendar = pd.read_csv(path_data + 'calendar.csv')\n\nsales_cal = sales_melt.merge(calendar[['date','wm_yr_wk','weekday','wday','month','year','d']], how='left', on=['d'])\nsales_cal['date'] = pd.to_datetime(sales_cal.date, format=\"%Y-%m-%d %H:%M:%S\")\nsales_cal['day'] = sales_cal.date.dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,3,figsize=(25,5))\n\nweekdays = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\npd.concat(\n    [sales_cal[sales_cal.cat_id=='HOBBIES'].groupby('weekday')['sales'].agg('mean').reindex(weekdays), \n     sales_cal[sales_cal.cat_id=='HOUSEHOLD'].groupby('weekday')['sales'].agg('mean').reindex(weekdays), \n     sales_cal[sales_cal.cat_id=='FOODS'].groupby('weekday')['sales'].agg('mean').reindex(weekdays)],\n    axis=1).plot.bar(rot=45, ax=axes[0])\naxes[0].set_xlabel('Weekday', fontsize=15)\naxes[0].set_ylabel('Average sales', fontsize=15)\naxes[0].tick_params(labelsize=12)\naxes[0].legend(['HOBBIES', 'HOUSEHOLD', 'FOODS'], fontsize=15)\n\npd.concat(\n    [sales_cal[sales_cal.cat_id=='HOBBIES'].groupby('month')['sales'].agg('mean'), \n     sales_cal[sales_cal.cat_id=='HOUSEHOLD'].groupby('month')['sales'].agg('mean'), \n     sales_cal[sales_cal.cat_id=='FOODS'].groupby('month')['sales'].agg('mean')],\n    axis=1).plot.bar(rot=45, ax=axes[1])\naxes[1].xaxis.set_label_text('')\naxes[1].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], fontsize=15)\naxes[1].set_xlabel('Month', fontsize=15)\naxes[1].set_ylabel('Average sales', fontsize=15)\naxes[1].tick_params(labelsize=12)\naxes[1].get_legend().remove()\n\nsales_cal.groupby('day')['sales'].agg('mean').plot(kind='bar', rot=0, ax=axes[2])\naxes[2].xaxis.set_label_text('')\naxes[2].set_xlabel('Day of month', fontsize=15)\naxes[2].set_ylabel('Average sales', fontsize=15)\naxes[2].tick_params(labelsize=12)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> more sales on weekends, especially for food"},{"metadata":{"trusted":true},"cell_type":"code","source":"prop_nonzero = sales_train_val[d_cols].astype(bool).sum(axis=0).reset_index().drop('index', axis=1)/sales_train_val.shape[0]\nax = prop_nonzero.plot(figsize=(13, 6))\nax.set_xlabel(\"day\", fontsize=15)\nax.set_ylabel(\"Proportion at least 1 sale\", fontsize=15)\nax.tick_params(labelsize=15)\nax.get_legend().remove()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,3,figsize=(25,5))\n\nsales_melt.groupby(['cat_id']).agg({'sales': 'mean'}).plot(kind='bar', rot=0, ax=axes[0])\naxes[0].xaxis.set_label_text('')\naxes[0].set_xticklabels(['Food','Hobbies','Household'], fontsize=15)\naxes[0].set_ylabel('Mean sell price', fontsize=15)\naxes[0].tick_params(labelsize=12)\naxes[0].get_legend().remove()\n\nsales_melt.groupby(['dept_id']).agg({'sales': 'mean'}).plot(kind='bar', rot=45, ax=axes[1])\naxes[1].xaxis.set_label_text('')\naxes[1].set_xlabel('Department', fontsize=15)\naxes[1].set_ylabel('Mean sell price', fontsize=15)\naxes[1].tick_params(labelsize=12)\naxes[1].get_legend().remove()\n\nsales_melt.groupby(['store_id']).agg({'sales': 'mean'}).plot(kind='bar', rot=45, ax=axes[2])\naxes[2].xaxis.set_label_text('')\naxes[2].set_xlabel('Store', fontsize=15)\naxes[2].set_ylabel('Mean sell price', fontsize=15)\naxes[2].tick_params(labelsize=12,rotation=0)\naxes[2].get_legend().remove()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sell prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sell_prices.wm_yr_wk-=sell_prices.wm_yr_wk.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nfood, = plt.plot(sell_prices[sell_prices['item_id'].str.match('FOOD')].groupby('wm_yr_wk')['sell_price'].mean(), color='blue')\nhobbies, = plt.plot(sell_prices[sell_prices['item_id'].str.match('HOBBIES')].groupby('wm_yr_wk')['sell_price'].mean(), color='orange')\nhousehold, = plt.plot(sell_prices[sell_prices['item_id'].str.match('HOUSEHOLD')].groupby('wm_yr_wk')['sell_price'].mean(), color='green')\n_ =plt.xlabel('week', fontsize=15)\n_ =plt.ylabel('mean sell price', fontsize=15)\n_ =plt.tick_params(labelsize=12)\n_ =plt.title('Evolution of overall sale prices', fontsize=16)\nplt.legend([food, hobbies, household], ['Food', 'Hobbies', 'Household'], fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_range_per_id = sell_prices.groupby('item_id')['sell_price'].agg(np.ptp)\nplt.figure(figsize=(8,3))\n_ = plt.hist(price_range_per_id, bins=100)\n_ =plt.xlabel('max-min sell price per id', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> for individual items, variations in sell prices are quite small"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,4))\nfor i, item in enumerate(['HOBBIES_1_001','HOBBIES_1_002','HOBBIES_1_003']):\n    ax = fig.add_subplot(1,3,i+1)\n    for store in sell_prices['store_id'].unique():\n        prices_item_store = sell_prices[(sell_prices.item_id==item) & (sell_prices.store_id==store)]\n        ax.plot(prices_item_store.wm_yr_wk, prices_item_store.sell_price, label=store)\n    if i==2:\n        ax.legend()\n    ax.set_title(item, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> item prices are mostly constant with only exceptional outliers (maybe special offers?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# investigate relationship between price and number of sales\n\nitems = ['HOBBIES_1_001','HOBBIES_1_002','HOBBIES_1_003']\nstores = ['TX_1','CA_3','WI_2']\n\nfor i in range(3):\n    item=items[i]\n    store=stores[i]\n    \n    sales_item_store = sales_melt[(sales_melt.item_id==item) & (sales_melt.store_id==store)]\n    sales_item_store = sales_item_store.merge(calendar[['date','wm_yr_wk','weekday','wday','month','year','d']], how='left', on=['d'])\n    sales_item_store = sales_item_store.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'])\n    \n    fig, ax1 = plt.subplots()\n    ax1.plot(sales_item_store.d, sales_item_store.sell_price, color='red')\n    ax1.tick_params(axis='y', labelcolor='red')\n    ax1.set_ylabel('sell price', color='red')\n\n    ax2 = ax1.twinx()\n    ax2.plot(sales_item_store.d, sales_item_store.sales, color='green')\n    ax2.tick_params(axis='y', labelcolor='green')\n    ax2.set_ylabel('sales', color='green')\n    plt.title('item: ' + item + ', store: ' + store)\n\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> there seems to be no obvious relationship between sell price and sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove temporary variables from workspace again\ndel prop_nonzero, sales_cal, sales_melt, sales_train_val, sell_prices, price_range_per_id, sales_item_store, prices_item_store","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%whos DataFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transform data"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_day = 1200 #902  # 350\ndf_train_val_test = read_and_transform(start_day)","execution_count":6,"outputs":[{"output_type":"stream","text":"###### Transforming into melted and merged data format...\nMemory usage of dataframe is 622.84 MB\nMemory usage after optimization is: 292.17 MB\nDecreased by 53.1%\nMemory usage of dataframe is 32.57 MB\nMemory usage after optimization is: 17.62 MB\nDecreased by 45.9%\nMemory usage of dataframe is 31.76 MB\nMemory usage after optimization is: 19.40 MB\nDecreased by 38.9%\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-5620dbeb5332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1200\u001b[0m \u001b[0;31m#902  # 350\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train_val_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-a11c177937cb>\u001b[0m in \u001b[0;36mread_and_transform\u001b[0;34m(start_day)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m### Merge trains and submission dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mdf_train_val_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_submission\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdf_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   5268\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5239\u001b[0m         \"\"\"\n\u001b[1;32m   5240\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5241\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5249\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1911\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[0;32m-> 1913\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_can_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m         )\n\u001b[1;32m   1915\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3340\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3341\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train_val_test.to_csv('df_train_val_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_val_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_val_test[df_train_val_test.part=='train_val'].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_val_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_val_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if dates are correct:\n# public leaderboard data should begin with d_1914 and 2016-04-25\ndf_train_val_test[df_train_val_test.part=='public_leaderboard'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature-engineering"},{"metadata":{},"cell_type":"markdown","source":"Possible features to implement:\n* lag by one week, one month, one year\n* rolling mean last month, last week\n* these two combined, i.e. mean for N last days\n* price trend (current price vs. mean of ...)\n* sell trend (#sells in last X days compared to previous X days)\n* in the days before a holidays, there are probably a lot of sells, one could add a feature like 'days from holiday' or 'preceding holiday'\n* encode sudden changes in price (see EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# lag-features\n# maybe create aggregate features above rather here (as 'groupby('id')')\n#features = ['sales','sell_price']\nfeatures = ['sales']\nlags = [7, 28]\nfor feature in features:\n    for lag in lags:\n        df_train_val_test['{}_lag{}'.format(feature, lag)] = df_train_val_test[['id','sales','sell_price']].groupby('id')[feature].transform(lambda x: x.shift(lag))\n        \nlag_features = [col for col in df_train_val_test.columns if 'lag' in col]\n\n# transform into rolling window features\nwindows = [7, 28]\nfor feature in lag_features:\n    for window in windows:\n        df_train_val_test['{}_mean_window{}'.format(feature, window)] = df_train_val_test[[\"id\", feature]].groupby('id')[feature].transform(lambda x: x.rolling(window).mean())\n        \n# drop nan that occur in the new columns\ndf_train_val_test.dropna(inplace = True)\ndf_train_val_test = reduce_mem_usage(df_train_val_test)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlags = [7]\nfor feature in temp_features:\n    for lag in lags:\n        df_train_val_test['{}_lag{}'.format(feature, lag)] = df_train_val_test[['id'] + temp_features].groupby('id')[feature].transform(lambda x: x.shift(lag))\n\nlag_features = [col for col in df_train_val_test.columns if 'lag' in col]\n\n# transform into rolling window features\nwindows = [7]\nfor feature in lag_features:\n    for window in windows:\n        df_train_val_test['{}_mean_window{}'.format(feature, window)] = df_train_val_test[[\"id\", feature]].groupby('id')[feature].transform(lambda x: x.rolling(window).mean())\n        \n# drop temporary features\ndf_train_val_test.drop(temp_features, axis=1, inplace=True)\n\n# drop nan that occur in the new columns\ndf_train_val_test.dropna(inplace = True)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_features(data):\n    \n    # rolling sales features\n    data['lag_t28'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28)).astype('float16')\n    data['lag_t29'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(29)).astype('float16')\n    data['lag_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(30)).astype('float16')\n    data['rolling_mean_t7'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).mean()).astype('float16')\n    data['rolling_std_t7'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).std()).astype('float16')\n    data['rolling_mean_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).mean()).astype('float16')\n    data['rolling_mean_t90'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(90).mean()).astype('float16')\n    data['rolling_mean_t180'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(180).mean()).astype('float16')\n    data['rolling_std_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).std()).astype('float16')\n    data['rolling_skew_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).skew()).astype('float16')\n    data['rolling_kurt_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).kurt()).astype('float16')\n    \n    # price features\n    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1)).astype('float16')\n    data['lag_price_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(7)).astype('float16')\n    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1']).astype('float16')\n    data['price_change_t7'] = (data['lag_price_t7'] - data['sell_price']) / (data['lag_price_t7']).astype('float16')\n    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max()).astype('float16')\n    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365']).astype('float16')\n    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std()).astype('float16')\n    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std()).astype('float16')\n    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n    \n    data['sell_price'] = data.sell_price.astype('float16')\n    \n    # add holidays\n    holidays = [str(year) + '-12-25' for year in range (2011, 2016)] + [str(year) + '-01-01' for year in range (2011, 2017)]\n    holidays = holidays + ['2011-11-24', '2012-11-22', '2013-11-28', '2014-11-27', '2015-11-26']\n    holidays = pd.to_datetime(holidays)\n\n    data['is_holiday'] = 0\n    data.loc[data.date.isin(holidays),'is_holiday']=1\n    data['is_holiday'] = data['is_holiday'].astype('int8')\n\n    return data\n\ndf_train_val_test = add_features(df_train_val_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_val_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_val_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_val_test.memory_usage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train model"},{"metadata":{},"cell_type":"markdown","source":"To do:\n* parameter tuning\n* model stacking\n* idea: read in preprocessed data by chunks and train model iteratively"},{"metadata":{"trusted":true},"cell_type":"code","source":"%whos DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n### split into training and validation set with 80% training and 20% validation\n# i.e. 80% of days into training and 20% of days into validation set\nndays_train_val = df_train_val_test[df_train_val_test.part=='train_val'].date.nunique()\ndays_train = round(ndays_train_val*0.8)\ndays_val = ndays_train_val-days_train\nprint('Using {} days for training and {} days for validation, {} days in total'.format(days_train,days_val,ndays_train_val))\nlast_day_train = str(df_train_val_test.date.unique()[days_train-1])\n\nx_train = df_train_val_test[df_train_val_test['date'] <= last_day_train]\nx_val = df_train_val_test[(df_train_val_test['date'] > last_day_train) & (df_train_val_test['date'] < str(df_train_val_test[df_train_val_test.part=='public_leaderboard'].date.iloc[0]))]\ny_train = x_train['sales']\ny_val = x_val['sales']\nx_train.drop(drop_features, axis=1, inplace=True)\nx_val.drop(drop_features, axis=1, inplace=True)\ntest = df_train_val_test[df_train_val_test.part==\"public_leaderboard\"]\nx_test = test.drop(drop_features, axis=1)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose first day of the validation set (here using the last 28 days for validation)\nfirst_day_val = '2016-03-27'\n\n#drop_features = ['d', 'date', 'id', 'part', 'sales','wm_yr_wk']\ndrop_features = ['d', 'date', 'id', 'part', 'sales', 'wm_yr_wk']\n\n# going to evaluate with the last 28 days\nx_train = df_train_val_test[df_train_val_test['date'] < first_day_val]\ny_train = x_train['sales']\nx_val = df_train_val_test[(df_train_val_test['date'] >= first_day_val) & (df_train_val_test['date'] <= '2016-04-24')]\ny_val = x_val['sales']\nx_train.drop(drop_features, axis=1, inplace=True)\nx_val.drop(drop_features, axis=1, inplace=True)\ntest = df_train_val_test[(df_train_val_test['date'] > '2016-04-24')]\nx_test = test.drop(drop_features, axis=1)\n\ndel df_train_val_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%whos DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%whos DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n### Parameter tuning\n# Define searched space\nhyper_space_lgbm = {'objective': 'regression',\n                   'metric':'rmse',\n                   'boosting':'gbdt',\n                   'n_estimators': hp.choice('n_estimators', [50, 100, 200, 500]),\n                   'learning_rate': hp.choice('learning_rate', [.01, .02, .03, .1, .2]),\n                   'num_leaves': hp.choice('num_leaves', [25, 32, 50, 75, 100, 125, 150, 225, 250, 350]),\n                   'colsample_bytree': hp.choice('colsample_bytree', [.5, .6, .7, .8, .9, .95, 1]),\n                   'subsample': hp.choice('subsample', [.7, .8, .85, .9, .95, 1]),\n                   'max_depth':  hp.choice('max_depth', [3, 5, 8, 10, 12, 15]),\n                   'reg_alpha': hp.choice('reg_alpha', [.04, 0.7, .1, .2, .3, .4, .5, .6, .7]),\n                   'reg_lambda':  hp.choice('reg_lambda', [.04, .07, .1, .2, .3, .4, .5, .6]), \n                   'feature_fraction':  hp.choice('feature_fraction', [.6, .7, .8, .9, 1]), \n                   'bagging_frequency':  hp.choice('bagging_frequency', [.3, .4, .5, .6, .7, .8, .9]),\n                   'min_split_gain': hp.choice('min_split_gain', [.01, .02, .05]),\n                   'min_child_weight': hp.choice('min_child_weight', [10, 20, 30, 40])}\n\nlgtrain = lightgbm.Dataset(X_train, label=Y_train)\nlgval = lightgbm.Dataset(X_valid, label=Y_valid)\n\ndef evaluate_metric(params):\n    \n    model_lgb = lightgbm.train(params, lgtrain, 600, \n                          valid_sets=[lgtrain, lgval], early_stopping_rounds=100, \n                          verbose_eval=300)\n\n    pred = model_lgb.predict(X_valid, num_iteration=1000)\n    \n    score = rmse(pred, Y_valid)\n    \n    print(score, params)\n \n    return {\n        'loss': score,\n        'status': STATUS_OK,\n        'stats_running': STATUS_RUNNING\n    }\n\ndef rmse(y_pred, y):\n    return np.sqrt(np.mean(np.square(y - y_pred)))\n\n# Trail\ntrials = Trials()\n\n# Set algorithm parameters\nalgo = partial(tpe.suggest, \n               n_startup_jobs=-1)\n\n# Seting the number of Evals\nMAX_EVALS= 30\n\n# Fit Tree Parzen Estimator\nbest_vals = fmin(evaluate_metric, space=hyper_space_lgbm, verbose=1,\n                 algo=algo, max_evals=MAX_EVALS, trials=trials)\n\n# Print best parameters\nbest_params_lgbm = space_eval(hyper_space_lgbm, best_vals)\n\nprint(\"BEST PARAMETERS: \" + str(best_params_lgbm))\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### lgb model\n\n# define random hyperparammeters\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10, \n    'colsample_bytree': 0.75}\n\ntrain_set = lgb.Dataset(x_train, y_train)\ndel x_train, y_train\nval_set = lgb.Dataset(x_val, y_val)\n\nmodel = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\n\nval_pred = model.predict(x_val)\nval_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\nprint(f'Our val rmse score is {val_score}')\ny_test_pred = model.predict(x_test)\n\n# Plot feature importance\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,1,figsize=(20,10))\nlgb.plot_importance(model, ax=ax)\n\n# last score: 2.125186067233196 (start_day = 1200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### create submission file\nsubmission = pd.read_csv(path_data + 'sample_submission.csv')\ntest['sales'] = y_test_pred\npredictions = test[['id', 'date', 'sales']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'sales').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}